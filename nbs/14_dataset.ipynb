{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset\n",
    "\n",
    "> End-to-end functions taking in centerline-stroke SVG's and outputting deltas in Stroke-3 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from singleline_stroke3.display import *\n",
    "from singleline_stroke3.strokes import *\n",
    "from singleline_stroke3.svg import *\n",
    "from singleline_stroke3.transforms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk Processing SVG's into Stroke-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def enumerate_files(input_dir):\n",
    "    \"\"\"Find all the files within a directory (non-recursively)\"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(input_dir):\n",
    "        if os.path.isfile(os.path.join(input_dir, file)):\n",
    "            files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def svgs_to_deltas(\n",
    "    input_dir,\n",
    "    output_dir=None,\n",
    "    target_size=200,\n",
    "    total_n=1000,\n",
    "    min_n=3,\n",
    "    epsilon=1.0,\n",
    "    limit=None,\n",
    "):\n",
    "    if output_dir:\n",
    "        svg_dir = os.path.join(output_dir, \"svg\")\n",
    "        png_dir = os.path.join(output_dir, \"png\")\n",
    "        for d in [svg_dir, png_dir]:\n",
    "            if not os.path.isdir(d):\n",
    "                os.makedirs(d)\n",
    "\n",
    "    all_files = enumerate_files(input_dir)\n",
    "    print(f\"found {len(all_files)} in {input_dir}\")\n",
    "    dataset = []\n",
    "    for i, fname in enumerate(all_files):\n",
    "        if limit and i > limit:\n",
    "            break\n",
    "        input_fname = os.path.join(input_dir, fname)\n",
    "\n",
    "        try:\n",
    "            rescaled_strokes = svg_to_strokes(input_fname, total_n=total_n, min_n=min_n)\n",
    "\n",
    "            joined_strokes, _ = merge_until(rescaled_strokes, dist_threshold=15.0)\n",
    "            spliced_strokes, _ = splice_until(joined_strokes, dist_threshold=40.0)\n",
    "\n",
    "            print(\n",
    "                f\"{fname}: {len(rescaled_strokes)} strokes -> {len(joined_strokes)} joined -> {len(spliced_strokes)} spliced\"\n",
    "            )\n",
    "\n",
    "            deltas = stroke_rdp_deltas(spliced_strokes, epsilon=epsilon)\n",
    "            dataset.append(deltas)\n",
    "\n",
    "            # monitor number of points before/after applying RDP path simplification algorithm\n",
    "            raw_points = np.vstack(rescaled_strokes).shape[0]\n",
    "            rdp_points = deltas.shape[0]\n",
    "            print(f\"{input_fname} points: raw={raw_points}, rdp={rdp_points}\")\n",
    "\n",
    "            if output_dir:\n",
    "\n",
    "                def new_suffix(subdir, fname, suffix):\n",
    "                    sd = os.path.join(output_dir, subdir)\n",
    "                    if not os.path.isdir(sd):\n",
    "                        os.makedirs(sd)\n",
    "                    return os.path.join(sd, fname.replace(\".svg\", suffix))\n",
    "\n",
    "                final_n_strokes = len(spliced_strokes)\n",
    "                subdir = f\"png/{final_n_strokes:02d}\"\n",
    "                plot_strokes(\n",
    "                    rescaled_strokes, fname=new_suffix(subdir, fname, \".0_strokes.png\")\n",
    "                )\n",
    "                plot_strokes(\n",
    "                    joined_strokes, fname=new_suffix(subdir, fname, \".1_joined.png\")\n",
    "                )\n",
    "                plot_strokes(\n",
    "                    spliced_strokes, fname=new_suffix(subdir, fname, \".2_spliced.png\")\n",
    "                )\n",
    "                plot_strokes(\n",
    "                    deltas_to_strokes(deltas),\n",
    "                    fname=new_suffix(subdir, fname, \".3_deltas.png\"),\n",
    "                )\n",
    "\n",
    "                # raw_output_fname = new_suffix('svg', fname, \".raw.svg\")\n",
    "                # with open(raw_output_fname, \"w\", encoding=\"utf-8\") as raw_out:\n",
    "                #     raw_dwg = render_strokes(rescaled_strokes, target_size=target_size)\n",
    "                #     raw_dwg.write(raw_out, pretty=True)\n",
    "                #     print(f\"\\twrote {raw_output_fname}\")\n",
    "\n",
    "                # preproc_output_fname = new_suffix('svg', fname, \".preproc.svg\")\n",
    "                # with open(preproc_output_fname, \"w\", encoding=\"utf-8\") as preproc_out:\n",
    "                #     preproc_dwg = render_deltas(deltas, target_size=target_size)\n",
    "                #     preproc_dwg.save(preproc_output_fname)\n",
    "                #     print(f\"\\twrote {preproc_output_fname}\")\n",
    "        except Exception as e:\n",
    "            print(f\"error processing idx={i} input_fname={input_fname}: {e}\")\n",
    "            # raise e\n",
    "    return np.array(dataset, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dir = '../data/svg/'\n",
    "# output_dir = '../outputs'\n",
    "\n",
    "# # debug: only run for the first 10 files\n",
    "# limit = 10\n",
    "\n",
    "# _ = svgs_to_deltas(input_dir, output_dir, limit=limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial_dataset = svgs_to_deltas(input_dir, output_dir, limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(partial_dataset)\n",
    "# np.savez('../outputs/subset.npz', partial_dataset, encoding='latin1', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sidebar:** Visualizing all the images in the dataset (up to N strokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from moviepy.editor import *\n",
    "\n",
    "# imgs_01 = sorted(enumerate_files(\"../outputs_segmented/png/01/\"))\n",
    "# abs_01 = [os.path.join(\"../outputs_segmented/png/01\", f) for f in imgs_01]\n",
    "# imgs_02 = sorted(enumerate_files(\"../outputs_segmented/png/02/\"))\n",
    "# abs_02 = [os.path.join(\"../outputs_segmented/png/02\", f) for f in imgs_02]\n",
    "# imgs_03 = sorted(enumerate_files(\"../outputs_segmented/png/03/\"))\n",
    "# abs_03 = [os.path.join(\"../outputs_segmented/png/03\", f) for f in imgs_03]\n",
    "# imgs_04 = sorted(enumerate_files(\"../outputs_segmented/png/04/\"))\n",
    "# abs_04 = [os.path.join(\"../outputs_segmented/png/04\", f) for f in imgs_04]\n",
    "\n",
    "# all_fnames = abs_01 + abs_02 + abs_03 + abs_04\n",
    "\n",
    "# new_clip = ImageSequenceClip(all_fnames, fps=20)\n",
    "# new_clip.write_videofile(\"new_file_fps20.mp4\")\n",
    "\n",
    "# new_clip = ImageSequenceClip(all_fnames, fps=24)\n",
    "# new_clip.write_videofile(\"new_file_fps24.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def stroke_summary_df(dataset):\n",
    "    summary = [\n",
    "        {\n",
    "            \"idx\": i,\n",
    "            \"num_points\": len(deltas),\n",
    "            \"num_strokes\": len(deltas_to_strokes(deltas)),\n",
    "        }\n",
    "        for i, deltas in enumerate(dataset)\n",
    "    ]\n",
    "    # by_num_strokes = sorted(summary, key=lambda k: k[\"num_strokes\"], reverse=True)\n",
    "    df = pd.DataFrame(summary)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_train_val(full_dataset, output_fname, split_ratio=0.8, max_strokes=None, max_points=None, min_points=None):\n",
    "    full_df = stroke_summary_df(full_dataset)\n",
    "    _df = full_df\n",
    "    if max_strokes:\n",
    "        _df = full_df[full_df.num_strokes <= max_strokes]\n",
    "    if max_points:\n",
    "        _df = full_df[full_df.num_points <= max_points]\n",
    "    if min_points:\n",
    "        _df = full_df[full_df.num_points <= min_points]\n",
    "\n",
    "    shuffled_df = _df.sample(frac=1)\n",
    "\n",
    "    train_size = int(len(shuffled_df) * split_ratio / 100) * 100\n",
    "    val_size = len(shuffled_df) - train_size\n",
    "    print(train_size, val_size, len(shuffled_df))\n",
    "\n",
    "    shuffled_df_train = full_dataset[list(shuffled_df[:train_size].idx)]\n",
    "    shuffled_df_val = full_dataset[list(shuffled_df[train_size:].idx)]\n",
    "    print(len(shuffled_df_train), len(shuffled_df_val))\n",
    "\n",
    "    np.savez(\n",
    "        output_fname,\n",
    "        train=shuffled_df_train,\n",
    "        valid=shuffled_df_val,\n",
    "        test=shuffled_df_val,\n",
    "        encoding=\"latin1\",\n",
    "        allow_pickle=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
