# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_embeddings.ipynb.

# %% auto 0
__all__ = ['DEFAULT_BATCH_SIZE', 'sketchbook_dataloaders', 'sketchbook_resnet34', 'batch_fnames_and_images', 'predict_embeddings',
           'Hook', 'embed_dir']

# %% ../nbs/01_embeddings.ipynb 4
import math

import pandas as pd
from fastai.vision.all import *

from .fileorg import *

# %% ../nbs/01_embeddings.ipynb 6
DEFAULT_BATCH_SIZE = 64


def sketchbook_dataloaders(sketchbooks_dir, **kwargs):
    path = Path(sketchbooks_dir)

    files = get_image_files(path)

    path_func = lambda p: os.path.basename(os.path.dirname(p))

    dataloaders = ImageDataLoaders.from_path_func(
        path,
        files,
        path_func,
        item_tfms=Resize(224),
        batch_size=DEFAULT_BATCH_SIZE,
        **kwargs
    )
    return dataloaders

# %% ../nbs/01_embeddings.ipynb 14
def sketchbook_resnet34(sketchbooks_dir, load_checkpoint=None):
    dataloaders = sketchbook_dataloaders(sketchbooks_dir)
    print(dataloaders.vocab)
    learn = vision_learner(dataloaders, resnet34, metrics=error_rate)
    if load_checkpoint:
        print(f"loading {load_checkpoint}")
        learn.load(load_checkpoint)
    return learn

# %% ../nbs/01_embeddings.ipynb 23
def batch_fnames_and_images(sketchbooks_dir):
    """
    Prepare data to compute embeddings over all images and store
    them along with a reference to the underlying filename.
    """
    ordered_dls = sketchbook_dataloaders(
        sketchbooks_dir, seed=42, shuffle=False, valid_pct=0.0
    )

    # compute the number of batches
    num_batches = math.ceil(len(ordered_dls.train.items) / DEFAULT_BATCH_SIZE)
    batched_fnames = [
        ordered_dls.train.items[i * 64 : (i + 1) * 64] for i in range(num_batches)
    ]
    print(
        f"total items: {len(ordered_dls.train.items)}, num batches: {len(batched_fnames)}"
    )
    return batched_fnames, ordered_dls

# %% ../nbs/01_embeddings.ipynb 25
def predict_embeddings(model, xb):
    # import pdb
    # pdb.set_trace()
    with torch.no_grad():
        with Hook(model[-1][-2]) as hook:
            output = model.to("cpu").eval()(xb.to("cpu"))
            act = hook.stored
    return act.cpu().numpy()


class Hook:
    def __init__(self, m):
        self.hook = m.register_forward_hook(self.hook_func)

    def hook_func(self, m, i, o):
        self.stored = o.detach().clone()

    def __enter__(self, *args):
        return self

    def __exit__(self, *args):
        self.hook.remove()

# %% ../nbs/01_embeddings.ipynb 29
def embed_dir(input_dir, learner):
    batched_fnames, ordered_dls = batch_fnames_and_images(input_dir)
    with torch.no_grad():
        for i, batch in enumerate(zip(batched_fnames, ordered_dls.train)):
            batched_fnames, (x, y) = batch
            bs = len(batched_fnames)
            assert bs == x.shape[0]
            assert bs == y.shape[0]

            activations = predict_embeddings(learner.model, x)
            assert bs == activations.shape[0]

            for j in range(bs):
                x_j = x[j]
                y_j = y[j]
                fname_j = batched_fnames[j]
                emb_j = activations[j]
                # label: what parent dir exists in the dataset we're processing
                label_j = ordered_dls.vocab[y_j]
                # pred_label: prediction made relative to the vocab of the learner's model
                # (may be different than what's in the dataloader we're using for input).
                pred_label_j, pred_idx_j, pred_probs_j = learner.predict(x_j.cpu())
                yield {
                    "idx": j + i * bs,
                    "abs_fname": fname_j,
                    "rel_fname": str(fname_j).replace(str(input_dir) + "/", ""),
                    "label": label_j,
                    "pred_label": pred_label_j,
                    "pred_idx": pred_idx_j.cpu().numpy(),
                    "pred_probs": ",".join(
                        [f"{p:04f}" for p in pred_probs_j.cpu().numpy()]
                    ),
                    "emb_csv": ",".join([str(f) for f in list(emb_j)]),
                }
