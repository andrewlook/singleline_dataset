# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/50_tokenize.ipynb.

# %% auto 0
__all__ = ['Stroke3Tokenizer']

# %% ../nbs/50_tokenize.ipynb 4
import math
import json

import faiss
import pandas as pd
from fastai.vision.all import *

from .embeddings import *
from .display import *
from .transforms import *
from .fileorg import *

# %% ../nbs/50_tokenize.ipynb 14
class Stroke3Tokenizer(object):
    def __init__(self, centroids_zero, centroids_one):
        self.centroids_zero = centroids_zero
        print(self.centroids_zero.shape)
        self.kmeans_zero = faiss.IndexFlatL2(self.centroids_zero.shape[1])
        self.kmeans_zero.add(self.centroids_zero)

        self.centroids_one = centroids_one
        self.kmeans_one = faiss.IndexFlatL2(self.centroids_one.shape[1])
        self.kmeans_one.add(self.centroids_one)

    def save(self, base_dir, extra_suffix=""):
        CENTROIDS_FNAME = base_dir / f"stroke3_centroids{extra_suffix}.json"
        with open(CENTROIDS_FNAME, "w") as outfile:
            payload = {
                "centroids_zero": self.centroids_zero.tolist(),
                "centroids_one": self.centroids_one.tolist(),
            }
            json.dump(payload, outfile, indent=2)
        print(f"wrote {CENTROIDS_FNAME}")

    @staticmethod
    def load(base_dir, extra_suffix=""):
        CENTROIDS_FNAME = base_dir / f"stroke3_centroids{extra_suffix}.json"
        with open(CENTROIDS_FNAME, "r") as infile:
            payload = json.load(infile)
        centroids_zero = np.array(payload["centroids_zero"], dtype=np.float32)
        centroids_one = np.array(payload["centroids_one"], dtype=np.float32)

        tokenizer = Stroke3Tokenizer(centroids_zero, centroids_one)
        return tokenizer

    def encode(self, input_deltas):
        """
        total vocabulary size is len(centroids_zero) + len(centroids_one).

        for a point with lift_pen=0, the "word" index in the vocabulary
        is equal to its position within the centroids_zero list.

        for a point with lift_pen=1, the "word" index in the vocabulary
        is equal to its position within the centroids_one list, PLUS
        the total length of the centroids_zero list.
        """
        D0, I0 = self.kmeans_zero.search(input_deltas, 1)
        D1, I1 = self.kmeans_one.search(input_deltas, 1)

        tokens = []
        for idx in range(input_deltas.shape[0]):
            row = input_deltas[idx]
            if row[2] == 0:
                tokens.append(I0[idx][0])
            elif row[2] == 1:
                tokens.append(I1[idx][0] + self.centroids_zero.shape[0])
            else:
                raise Exception("didnt find a 0 or 1 in the lift_pen column")
        return tokens

    def decode(self, tokens):
        num_zero = self.centroids_zero.shape[0]
        num_one = self.centroids_one.shape[0]
        decoded = []
        for tok in tokens:
            if tok < 0:
                raise Exception("invalid token index")
            if tok < num_zero:
                decoded.append(self.centroids_zero[tok])
            elif tok < (num_zero + num_one):
                decoded.append(self.centroids_one[tok - num_zero])
            else:
                raise Exception("invalid token index")
        return np.array(decoded)
